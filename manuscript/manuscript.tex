\documentclass[linenumber]{jdsart}
\usepackage{setspace}
 
\volume{0}
\issue{0}
\pubyear{2022}
\articletype{research-article}
\doi{0000}

\usepackage{siunitx} % For alignment of numbers
\sisetup{
    group-separator = {,},
    round-mode = places,
    round-precision = 2,
    output-decimal-marker = {.},
    table-number-alignment = center,
    table-figures-integer = 6,
    table-figures-decimal = 2,
    table-figures-uncertainty = 2
}

% image path
\graphicspath{{.}{./images}}

\usepackage{xcolor}
\newcommand{\dt}[1]{\textcolor{purple}{DT: (#1)}}
\newcommand{\jy}[1]{\textcolor{cyan}{JY: (#1)}}

\let\proglang=\textsf
%% \newcommand{\pkg}[1]{{\fontseries{m}\selectfont #1}}
%% \newcommand\code[2][black]{\textcolor{#1}{\texttt{#2}}}

\usepackage{comment}
\usepackage{booktabs, textgreek}
\usepackage{subcaption}
\usepackage{enumitem}

%% float control
\renewcommand\floatpagefraction{0.75}
% \renewcommand\topfraction{.8}
% \renewcommand\bottomfraction{.8}
% \renewcommand\textfraction{.2}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}


\doublespacing
\begin{document}

\begin{frontmatter}
  
\title{Principles for Open Data Curation: A Case Study with the New
York City 311 Service Request Data}
\runtitle{Principles for Open Data Curation}

\author[1]{\inits{D.}\fnms{David}~\snm{Tussey}}
\author[2]{\inits{J.}\fnms{Jun}~\snm{Yan}}
\address[1]{\institution{NYC DoITT}, \cny{USA}}
\address[2]{Department of Statistics,
  \institution{University of Connecticut}, \cny{USA}}



 \tableofcontents % Optional: Table of Contents
 \listoffigures % List of Figures
 \listoftables % List of Tables

\hyphenpenalty=950

\begin{abstract}
In the early 21st century, the open data movement began to transform 
societies and governments by promoting transparency,
innovation, and public engagement. The City of New York (NYC) has been at
the forefront of this movement since the enactment of the Open 
Data Law in 2012, leading to the creation of the NYC Open Data
portal. This portal currently hosts 2,700 datasets,
serving as a crucial resource for research across various domains, 
including health, urban development, and transportation. The 
success of this initiative highlights the importance of data 
curation in ensuring the utility and reliability of open datasets.


This paper examines the challenges of open data curation through a
case study of the NYC 311 Service Request (SR) dataset, addressing issues 
of data validity, consistency, and curation efficiency. Based on 
insights from this case study, we propose a set of data curation 
principles tailored for government\mbox{-}released open data. These principles 
aim to enhance data management practices and ensure 
the ongoing utility of open data. The paper concludes with 
actionable recommendations for enhancing data curation and outlines
general principles for the effective release of open data.

\end{abstract}

\begin{keywords}
  \kwd{Data cleansing}
  \kwd{Data Curation}
  \kwd{Data science}
  \kwd{NYC Open Data}
  \kwd{Open data}
  \kwd{Smart Cities}
  \kwd{Transparency}
\end{keywords}

\end{frontmatter}

\section{Introduction} 
\label{sec:intro}

In the early 21st century, the open data movement began 
to take shape, driven by the fundamental belief that 
freely accessible data can transform both societies and 
governments. This movement champions the principles
of transparency, innovation, and public engagement. 
A landmark in this journey was the launch of the United States'
\href{https://www.data.gov}{Data.gov} portal in 2009, a pioneering
platform in making government data widely accessible. Shortly afterwards,
the European Union followed suit, unveiling its
\href{https://data.europa.eu/euodp}{Open Data Portal} in 2012, further
cementing the movement's global reach. Furthermore, the World Bank's Open
Data initiative, initiated in 2010, stands out as a comprehensive
repository for global development data, available at
\href{https://data.worldbank.org}{World Bank Open Data}. 
These initiatives represent significant strides in democratizing data, 
removing barriers that once kept valuable information 
regarding government performance in silos. Their collective impact 
extends beyond mere data sharing to fostering a culture of openness 
that benefits individuals, communities, governments, and economies worldwide 
\citep{barns2016mine, wang2016adoption}.


The City of New York (NYC) has emerged as a leader in the open data movement,
marked by the enactment of the Open Data Law in 2012
\citep{zuiderwijk2014open}. This landmark legislation led to the
creation of the \href{https://opendata.cityofnewyork.us}{NYC Open Data
  portal}, which today hosts an impressive array of 2,700 datasets
across 80 different city agencies. This resource has become invaluable
for researchers across various fields and has significantly enhanced
local government transparency. Popular datasets include information on
restaurant health inspection violations, car crashes, high school and
college enrollment statistics, jail inmate charges, and the location
of City\mbox{-}wide free Internet access points. These datasets have been
applied in civil life in various ways, such as mapping car crashes
involving pedestrians and visualizing high school and college
enrollment trends. Furthermore, they have enabled significant research
across multiple domains, including health \citep{cantor2018facets, 
shankar2021data}, urban development \citep{neves2020impacts}, and
transportation \citep{gerte2019understanding}, aiding in the
understanding and addressing of complex urban challenges.


Data curation, the process of organizing, maintaining, and ensuring
the quality of datasets, plays a crucial role in maximizing the
utility of open data. Proper curation ensures that datasets remain
consistent, accurate, and useful for diverse applications. For example, 
well\mbox{-}curated data is essential for machine learning systems, which 
require high\mbox{-}quality data to produce reliable insights 
\citep{polyzotis2019data, jain2020overview}. A critical component of 
data curation is data cleaning, which involves identifying and rectifying 
inconsistencies, errors, and inaccuracies in datasets. While open data 
initiatives have made vast amounts of data available, ensuring the 
reliability and utility of such data hinges on rigorous data cleaning processes. 
Efficient usage of software tools can significantly streamline this 
aspect of curation \citep[e.g.,][]{cody2017cody, van2018statistical}. Poor 
curation may result in issues such as missing data, formatting errors, 
or inconsistent values, leading to biased or inaccurate outcomes 
\citep{geiger2020garbage}. This is particularly critical in domains 
where machine learning is applied to sensitive tasks, such as public 
health or policy \citep{rahm2000data}.


Research into data curation has explored these challenges in\mbox{-}depth. 
Among the earliest discussions, \citet{witt2009constructing} focused 
on developing data curation profiles tailored to specific contexts, 
setting a precedent for targeted data management strategies. 
Addressing broader challenges in data sharing and management, 
\citet{borgman2012conundrum} highlighted the complexities of 
research data distribution, emphasizing the need for robust 
strategies. This is complemented by \citet{hart2016ten}, who outlined 
essential principles for effective data management, particularly 
emphasizing meticulous curation practices. The utility of curated 
open data is vividly illustrated in public health and global challenges, 
where \citet{cantor2018facets} demonstrated the utility of curated 
data in evaluating community health determinants, and 
\citet{shankar2021data} observed its critical role during the 
COVID\mbox{-}19 pandemic in managing collective responses.


The contributions of this paper are twofold. First, we delve into
the specifics of data curation challenges using the NYC 311 Service
Request (SR) Data as a case study. This dataset serves as a prime 
example for examining key issues in data curation, including data 
validity, consistency, and curation efficiency. We illustrate these 
points with live examples drawn from our 
processing of the 311 SR data. Secondly, building upon insights 
gained from this case study, we propose a set of data curation 
principles tailored for government\mbox{-}released open data. These 
principles are designed to address the unique challenges 
and requirements observed in the curation of such datasets.


The paper is organized as follows. Section~\ref{sec:data} provides 
an overview of the history of the NYC 311 SR system and presents 
a summary of SR counts over a 2\mbox{-}year period from 2022 to 2023. 
Section~\ref{sec:issues} delves into specific data cleansing 
challenges affecting data quality and curation efficiency, 
including structural problems, adherence to the data dictionary, 
and the presence of missing, blank, or N/A entries. This section
 also investigates data field compliance with reference or acceptable values, 
highlights logical inconsistencies, and examines some concerning patterns 
in the data. We explore the balance between precision and accuracy 
and identify duplicate or redundant fields, along with observations 
on the \href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary}.

Section~\ref{sec:recommendations} offers 
practical recommendations for mitigating or resolving these issues, 
while Section~\ref{sec:discussion} encapsulates key insights and 
discusses the broader implications of our findings.


\section{NYC 311 Service Request (SR) Data} 
\label{sec:data}
The NYC 311 service, a critical component of New York City's public
engagement and service response framework, serves as a centralized hub
for non\mbox{-}emergency inquiries and requests. Introduced in 2003, the NYC
311 system was designed to streamline the city's response to
non\mbox{-}emergency issues, ranging from noise complaints to street
maintenance requests. Initially a phone\mbox{-}based call center, the system
evolved into a comprehensive data management platform handling
millions of requests annually. Key milestones since its launch in 2003
include the addition of online and mobile app channels in 2009, a
record high of 348,463 monthly service requests in August 2020 associated 
with the the COVID\mbox{-}19 pandemic, the 2021 expansion 
to include the MTA's city subway system (the biggest 
expansion in 311  history), and a record 3.23 million 
service requests in 2023. 

Today, the NYC 311 data system manages over 3 million service
requests (SRs) per year. This data is publicly accessible through the NYC Open Data
Portal, which provides tools for querying, grouping, aggregating,
geo\mbox{-}mapping, visualizing, and exporting results. The
\href{https://www.nyc.gov/content/oti/pages/}{NYC Office of Technology 
and Innovation (OTI)}, provides technical support for 
the 311 application, the open data infrastructure, mobile \& web applications, 
and the public\mbox{-}facing data portal. Input data for the 311 system 
is sourced from 16 different NYC Agencies. Some of these Agencies
utilize the core 311 software, while others transfer data to the
311 Open Data Warehouse from their in\mbox{-}house systems.


The impact of NYC 311 data extends beyond operational efficiency; it
has become instrumental in shaping City governance and community
engagement. Open data not only ensures governmental transparency
but also empowers researchers, civic developers, and the general
public. The data has been pivotal in providing advice on shelters
during emergencies, handling inquiries during the COVID\mbox{-}19 pandemic,
enforcing standards between landlords and tenants, reallocating taxi
routes based on analyses by the Taxi and Limousine Commission (TLC),
and improving responsiveness across City Agencies.


Our investigation utilizes a 311 SR dataset covering  2\mbox{-}years (2022-2023).
 This sample was collected on 15 September 2024, and is used 
 to conduct detailed analysis in areas such as data 
 consistency, data validity, redundant fields, etc.. Instructions 
 on downloading this file from the NYC Open Data Portal 
 are available in the supplemental material. Here are some 
 characteristics of the 311 SR dataset used in this analysis:

\begin{itemize}[left=1.5em]
	\item The dataset is approximately 3.7 GB in size and contains 
	over 6.4 million rows. It was exported as a CSV file.

	\item There are 41 columns of data for each row, with each
	row representing a single Service Request (SR). 
	
	\item There are four date fields: \texttt{created\_date}, \texttt{closed\_date}, 
	\texttt{resolution\_action\_updated\_date}, \& \texttt{due\_date}.
	
	\item There are two borough fields; one of which appears to be a duplicate.
		
	\item There are seven street fields; two pair of which appear to be duplicates
	
	\item In addition to  \texttt{incident\_address}, there are five additional location fields: 

	\texttt{latitude \& longitude}, \texttt{street\_name}, \texttt{landmark}, USGS State Plane 
	Coordinate System, and \texttt{block} \textbf{\#}.
	
	\item A free\mbox{-}form text field, \texttt{resolution\_description}, which 
	supports 930 characters of input, including commas and 
	special characters  which can be problematic for automated processing.
\end{itemize}


During the 10\mbox{-}year period from 2014 to 2023,  NYC's 311 system
experienced a 50\% increase in the number of SRs, 
reflecting a growing public reliance on and usage of 
the 311 system. (During this 10\mbox{-}year time-frame, the 
population of NYC grew just 0.6\%.). The most significant increase
in activity occurred in 2020, when the volume of requests 
surged to 3.23 million during the COVID\mbox{-}19 pandemic, 
illustrating the essential role of the 311 system in supporting 
NYC residents during times of crisis. This steady 
rise in SRs can be at least partially attributed to the increased 
accessibility of the 311 system via online and mobile 
platforms, as well as heightened public awareness of the service. 
While the spike in 2020 was exceptional, the broader trend 
indicates sustained growth in SR volume in future years,  
highlighting the system's expanding role in managing 
both routine City operations and extraordinary events.

Figure~\ref{fig:SRcountbyAgency} provides a breakdown of SRs by
the responsible Agency, showing the cumulative 
percentage of SRs handled by each
agency over the 2\mbox{-}year time-frame. Note that the
distribution of SRs is heavily concentrated among a few key 
agencies. The six largest agencies are:

\begin{enumerate}[left=1.5em]
    \item New York Police Department (NYPD): 43\% of all SRs
    \item Housing Preservation and Development (HPD): 21\%
    \item New York City Department of Sanitation (DSNY): 10\%
    \item Department of Transportation (DOT): 7\%
    \item Department of Environmental Protection (DEP): 5\%
    \item Department of Parks and Recreation (DPR): 4\%
\end{enumerate}

Collectively these six City Agencies handle over 90\% of the total SR 
volume, indicative of the critical role these Agencies play in managing
public concerns, ranging from noise complaints and housing issues to
sanitation and transportation problems. The remaining
10\% of SRs is distributed across 10 additional Agencies. 
This concentration of SRs to the ``big six'' Agencies 
underscores the necessity for optimal data handling and 
curation processes within these  high volume Agencies.

\begin{figure}[tbp]
	\centering
	\includegraphics[width = \textwidth]{SRs_by_Agency.pdf}
  	\caption{SR Counts by Agency with Cumulative Percentage}
	\label{fig:SRcountbyAgency}
\end{figure}

The dataset contains 210 different \texttt{complaint\_types}. 
Figure~\ref{fig:SR_complaints} illustrates how the Top 20 
\texttt{complaint\_type(s)} account for 70\% 
of all SRs. The distribution of complaints is skewed, 
with a small number of issues dominating total SR volume. 
Of special interest are noise\mbox{-}related complaints, of 
which there are eight different types including vehicle, 
residential, commercial, and street noise. When 
combined, noise\mbox{-}related SRs make up 22\% of all 
complaints; the most frequent issue in the NYC 311 system. 
Other prominent categories include illegal parking, heat/hot water 
complaints, and blocked driveways. The cumulative percentage curve 
reveals that the Top 20 \texttt{complaint\_type(s)} comprise 70\% of SRs,
while the remaining 190 other \texttt{complaint\_type(s)}  are spread thinly 
across the remaining 30\%. This suggests that improving 
responses to the most frequent complaints could have an 
outsized impact on overall service efficiency and resident 
satisfaction. This chart also provides insights into the 
operational pressures faced by those City Agencies 
responsible for handling these high-volume complaints.

\begin{figure}[tbp]
 \centering
  \includegraphics[width = \textwidth]{SR_by_Complaint_Type.pdf} 
  \caption{Top 20 complaint types and Cumulative Percentage} 
  \label{fig:SR_complaints}
\end{figure}


\section{Data Cleansing Issues} 
\label{sec:issues}
Data cleansing refers to the process of identifying and rectifying
errors, inconsistencies, and inaccuracies within datasets to ensure
they are of high quality and reliable for analysis
\citep{maletic2005data, hosseinzadeh2023data}. The process
typically involves removing duplicate records, handling missing or
incomplete data, correcting mislabeled or inaccurate entries, and
standardizing data formats \citep[e.g.,][]{cody2017cody,
  van2018statistical}. In the context of open data, cleansing is
especially important as open datasets often come from diverse,
often uncoordinated sources, leading to variations in data quality,
completeness, and consistency. Without thorough data cleansing, 
the utility of open data can be limited and perhaps 
untrustworthy, affecting its reliability for research, 
policy making, and innovation. The main purpose of cleansing 
open data is to ensure that it is accurate, consistent, and usable a
cross multiple analytical platforms and by various stakeholders with a 
myriad of purposes. Data cleansing improves the trustworthiness 
of the data and enables more accurate analysis, 
better decision making, and the improved integration of 
data into machine learning models or other systems.


Many quality criteria are employed to ensure high\mbox{-}quality data 
processing. One of the primary efforts is data validation, which includes 
several critical checks. For instance, mandatory fields must not be 
left empty, ensuring that key information is always captured. 
Additionally, certain fields must conform to specific data types, 
such as numeric, character, or date formats; typically 
outlined in a \href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary}. An 
essential aspect of validation is domain compliance, where 
fields must adhere to a predefined set of 
values such as statuses, state names, or zip codes. Structural 
errors can also play a significant role in data quality, particularly when 
naming conventions or data entries are inconsistent. Common issues 
include fields that do not appear in the Data Dictionary and  
inconsistent usage of data fields. Furthermore, redundant 
(or irrelevant) fields can clutter datasets, reducing efficiency, 
creating confusion, and introducing errors. Logical inconsistencies 
are another consideration, such as related fields that violate expected 
relationships, e.g. a ``due date'' that precedes the ``created date.'' 
Lastly, the balance between accuracy and precision is crucial, as both 
must be managed to ensure meaningful data.


We identify the presence of issues in the NYC 311 SR open dataset;
not attempting to solve them, but rather to highlight their scope, 
magnitude, and potential impact. We would recommend 
that a corrective solution should be undertaken only 
after further investigation as to the why and how such 
issues might come about, including  detailed discussions 
with the originating agency as to whether or not the issue is an 
actual error, correct,  or holds some other status.


\subsection{Structural Issues}
\label{sec:structural}
Structural issues refers to how data is organized, formatted, 
or presented within a dataset. They can make 
it difficult to analyze the data effectively and often require extensive 
data cleaning efforts. Here are three structural issues we encountered.
 
\paragraph{Undocumented Fields:} The 311 SR 
\href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary} 
identifies 41 data columns (fields) and provides related information 
for each. However, there are six additional fields that can be
optionally downloaded if manually selected. These fields are 
visible in the portal's ``Column manager'' widget, labeled with the prefix 
``@computed\_region.'' and include:

\begin{itemize}[left=1.5em]
    \item \texttt{zip\_codes}
    \item \texttt{community\_districts}
    \item \texttt{borough\_boundaries}
    \item \texttt{city\_council\_districts}
    \item \texttt{police\_precincts}
    \item \texttt{police\_precinct}
\end{itemize}

While one can likely infer the meaning of these ``computed'' 
fields, uncertainty remains as to their derivation and 
usage. These six fields are not included in
the \href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary}. Additionally, 
our analysis shows that these six ``computed'' fields have 
significant data validity issues, leading us to categorize these six fields 
as experimental and \textit{not for public use}. An update 
to the Data Dictionary would seem appropriate. Accordingly, 
we will not address these fields in our analysis. 

\paragraph{Representation of Missing data:} We found that the 
311 SR dataset exhibited inconsistent approaches when 
presenting missing data. We discovered many different 
representations of missing data: nulls, spaces, ``NA'', ``N/A'', 
and ``<NA>''. This variety of representations complicates programming
efforts, and increases the likelihood of introducing errors 
in data analysis (typically by search for or excluding missing fields 
and not aware of the various representations of such). A more consistent 
approach to treating missing data should be employed to address this issue.

\paragraph{Challenges with daylight saving time (DST):} The 311 SR 
dataset captures local NYC times. Accordingly, twice each 
year there are changes to the local time as a result of observing 
DST. This can introduce some unexpected results. For example, 
on both March 13, 2022 and March 12, 2023 there are no times 
between 1:59AM and 2:59AM. The 2:00AM hour on those dates 
is simply missing owing to the clocks moving forward; at 2:00AM 
it instantly becomes 3:00AM. This creates a gap in the hourly 
distribution of SRs on those days, and exaggerates the 
lifespan of SRs, albeit by a single hour. Similarly on both
November 6, 2022 and November 5, 2023, the 
hour of 1:00AM - 2:00AM is repeated twice. This can result 
in a bizarre scenario where, for example, an SR can 
be \textit{created} at 1:59AM and then be \textit{closed} 30 minutes later 
at 1:29AM; seemingly nonsensical. Utilizing UTC time 
for date\mbox{-}time stamps would help address this issue.


\subsection{Missing Data by Field}
\label{sec:blanks}
Understanding the absence of data by field is important 
when undertaking analysis. For example, if you wanted to 
determine if SRs were closed before or after their
\texttt{due\_date}, you would be challenged as 99.6\% of the
\texttt{due\_date} fields are blank. When sorting fields by
blank (or N/A values) they appear to divide into three groups:

\begin{itemize}[left=1.5em]
    \item \textbf{Mostly Empty}: 93-99.9\% blank 
    \item \textbf{Partially Empty}: 40-4\% blank
    \item \textbf{Few/None Empty}: 2-0\% blank
\end{itemize}

The \textit{Mostly Empty} category includes fields such as
\texttt{taxi\_company\_borough}, \texttt{due\_date},
\texttt{road\_ramp}, and \texttt{bridge\_highway\_name}.
The \textit{Partially Empty} category includes fields such as
\texttt{location\_type}, \texttt{landmark}, 
and \texttt{cross\_street\_1 \& \_2}. And the \textit{Few/None Empty} 
category includes \texttt{created\_date}, \texttt{complaint\_type},
\texttt{agency}, and \texttt{status}. For some analysis efforts, it may be
prudent to inquire further as to why some fields are 
mostly or almost always blank. Figure~\ref{fig:blank_fields} is 
a graphic depiction of total empty (blank \& N/As) for each 
field illustrating the grouping into 
the \textbf{Most}, \textbf{Partial}, and \textbf{Few/None} categories.

\begin{figure}[tbp]
	\centering
  	\includegraphics[width=\textwidth]{BlankFields.pdf}
	\caption{Number and Percentage of Empty/Blank Entries}
	\label{fig:blank_fields}
\end{figure}


\subsection{Validating Data for Acceptable Values}
\label{sec:domain}
Any analytic effort must ensure that fields containing invalid values 
are identified and isolated from the analysis. For example, the \texttt{latitude} 
and \texttt{longitude} fields were found to all fall within the 
geographic boundaries of New York City. Likewise, 
the \texttt{unique\_key} field was indeed unique, as required. 
Unfortunately, the \href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary} specifies very few domains of 
acceptable values for other fields. Nonetheless, the following fields were tested and found to comply with their expected domains, as determined by public 
usage and as evidenced in other historical datasets:

\begin{itemize}[left=1.5em]
    \item \texttt{address\_type}
    \item \texttt{status}
    \item \texttt{borough}
    \item \texttt{borough\_boundaries}
    \item \texttt{park\_borough}
    \item \texttt{data\_channel}
    \item \texttt{vehicle\_type}
    \item \texttt{city\_council\_district}
\end{itemize}

However,  some fields proved problematic as to their 
compliance with a domain of allowable values. These include: 

\paragraph{Zip Codes:}
\label{sec:zipcodesissues}
All \texttt{incident\_zip} zip codes should 
validate against the USPS database, which contains 
44,173 valid zip codes. We discovered that the \texttt{incident\_zip} 
field has 0.07\% invalid entries, resulting in 4,374 errors. This includes
zip codes such as ``12345'', ``10000'', and ``99999'. If you group 
the invalid zip code entries by percentage by agency, it 
mirrors the \textit{overall} breakdown of SRs by Agency, potentially 
indicating a systemic problem.


\paragraph{Created and Closed Dates:}
\label{sec:negativeduration}
With the \texttt{created\_date} and \texttt{closed\_date} fields, one 
might expect that these fields would be automatically populated by the  
application software. Thus when creating  or closing an SR 
the the software would automatically populate these fields. 
Unfortunately, this does not seem to be the case, as several anomalies 
exist in the \texttt{created\_date} and \texttt{closed\_date} fields that
an automated process would normally exclude, including:

\begin{itemize}[left=1.5em]
    \item SRs with a \texttt{closed\_date} that occurs before the 
    \texttt{created\_date} resulting a nonsensical ``negative duration''. (12,450)
    \item \texttt{created\_date(s)} and \texttt{closed\_date(s)} in 
    the far distant past, e.g. ``1900-01-01''. (8)
    \item \texttt{created\_date(s)} and \texttt{closed\_date(s)} that 
    match \textbf{to the second} creating an impossible ``zero duration''. (163,720)
    \item An unusually large number of SRs closed and/or created 
    exactly at midnight, to the second. (247,000)
\end{itemize}


\paragraph{Closed before Created (Negative Duration):}
Citizens, NYC Government Officials, and Agencies use the \texttt{created\_date}(s) and 
\texttt{closed\_date}(s) to measure the \textit{duration} of SRs, 
a useful surrogate to measure an agency's responsiveness. 
While duration is not directly present in the dataset, 
it is easily computed as the difference between
\texttt{closed\_date} and \texttt{created\_date}.  Duration is one of 
most frequently used 311 performance metrics 
and is frequently analyzed, such as determining if 
one NYC borough receives faster Agency responses than 
another borough for certain complaints. There are 
12,450 SRs where the \texttt{closed\_date} precedes the 
\texttt{created\_date}, generating nonsensical ``negative durations.'' 
Although this represents only 0.2\% of the dataset, these errors can 
significantly impact response time analyses. Eight SRs with extremely 
large negative durations, all originating from the 
Department of Homeless Services (DHS); all contain an entry 
of \textit{1900-01-01} as the \texttt{closed\_date}. This results 
in very large negative durations exceeding $-$44,601 
days (or 122 years). Many other SRs have negative durations 
in excess of $-$300+ days. Such anomalies, though rare, can 
significantly skew statistical results if not addressed during 
data cleansing. Accordingly, these SR rows are removed from our analysis. 

Excluding the extreme negative values (any that are <$-$730 days), 
Figure~\ref{fig:negative-duration-violin} shows the broad spread of 
negative duration SRs. While there are few outliers, the magnitude 
of these negative durations (some <$-$300 days) is troubling 
and can produce bizarre analytical results such as greatly 
distorting the mean response time for a specific complaint category.

\begin{figure}[tbp]
	 \centering
 	 \includegraphics[width=\textwidth]{negative_duration_SR_violin.pdf}
 \caption{Distribution of Negative Durations}
 \label{fig:negative-duration-violin}
\end{figure}

Table~\ref{tab:largest-errors} summarizes the 
negative durations of the largest magnitude (in days). The data 
suggests that the negative duration issue is predominantly a problem 
within the Department of Transportation (DOT), where 95\% of these 
errors occur.

\begin{table}[tbp]
  \centering
  \caption{Sample  Negative Durations [excluding extreme values that are <$-$730 days]}
   \label{tab:largest-errors}
 	\begin{tabular}{l l l r l}
    \toprule
    {created\_date} & {closed\_date} & {duration (days)}  & {agency} \\
    \midrule
    2023-01-27 14:40:00 & 2022-01-14 14:40:00 & $-$378 & DOT \\
    2023-01-18 10:06:00 & 2022-01-12 10:06:00 & $-$371 & DOT \\
    2023-01-27 14:36:00 & 2022-01-22 14:35:00 & $-$370 & DOT \\
    2023-01-11 11:10:00 & 2022-01-09 11:10:00 & $-$367 & DOT \\
    2023-12-18 03:13:00 & 2023-01-16 13:10:00 & $-$335 & DOT \\
    \bottomrule
    \end{tabular}
 \end{table}

\paragraph{Identical Created \& Closed Dates (Zero Durations):}
A more prevalent issue occurs when the \texttt{closed\_date} and 
\texttt{created\_date} have exactly the same date\mbox{-}time stamp, 
to the second. This creates a \textbf{zero duration} for the 
SR, which is nonsensical. There are 163,720 such zero duration 
SRs, representing 2.6\% of all non\mbox{-}blank data. 
Additionally, 99\% percent of these zero duration SRs 
occur within five agencies: Department of Health \& Mental 
Hygiene (DHMH), DOT, Department of Buildings (DOB), DSNY, and 
DEP. This distribution \textbf{does not} 
mirror the overall SR distribution by agency, suggesting 
an Agency\mbox{-}specific issue.
	
\paragraph{Created or Closed at Midnight:}
Another issue involves the unusually large number of 
SRs where the \texttt{created\_date} and \texttt{closed\_date} 
indicate ``created'' or ``closed'' exactly at midnight (00:00:00), 
to the second. Normally, SR creation and closure follows 
the work\mbox{-}day schedule, with most SRs created 
during working hours and fewer at night or in the 
early morning. However, there is a significantly higher 
number of SRs closed exactly at midnight, as well 
as a large number created exactly at midnight. The magnitude 
of this anomaly is well outside the 3$\sigma$ level as shown 
in Figure~\ref{fig:stacked}

\begin{figure}[tbp]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{2-year-trend_SRs_created_on_the_hour.pdf}
        \caption{SRs Created Exactly on the Hour}
        \label{fig:busiestcreated}
    \end{subfigure}
    \par\medskip
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{2-year-trend_SRs_closed_on_the_hour.pdf}
        \caption{SRs Closed Exactly on the Hour}
        \label{fig:busiestclosed}
    \end{subfigure}
    \caption{Comparison of SRs Created and Closed by Hour of the Day}
    \label{fig:stacked}
\end{figure}

These unusual patterns of SR creation and closure exactly at midnight 
suggest the presence of a bulk create/close software process, perhaps one
that automatically assigns time stamps of midnight (00:00:00) to 
large batches of SRs prior to sending the data to the 311 
data warehouse. This behavior distorts the calculated duration of these 
SRs as well as overall system usage by providing inaccurate 
dates. Further analysis of the distribution by Agency shows that over 92\% of 
the ``closed-exactly-at-midnight'' SRs come from just two 
Agencies: DOB (69\%) and DSNY  (23\%).

\paragraph{Resolution Action Update Date}
A similar issue occurs when an SR is modified (and the \texttt{resolution\_action\_update\_date} is altered) well 
after the \texttt{closed\_date}.  When an SR is 
updated, the 311 software automatically populates the \texttt{resolution\_action\_update\_date}. While it is certainly 
possible, and even routine, to update an SR after it is closed, 
some of these updates seem to be well beyond any 
normal or expected timeframe, with some ``updates'' 
occurring over 2 years later. As shown in Figure~\ref{fig:resolution-violin}, 
there are 10,265 SRs that were updated >30 days after the 
\texttt{closed\_date} (Not that Figure~\ref(fig:resolution-violin) excludes 
values >730 days, an arbitrary cutoff to exclude 
infeasible dates such as \textit{1900-01-01}. The chart 
highlights the distribution of these post-closure 
 updates, with a notable concentration of updates occurring within 
the 30\mbox{-}to\mbox{-}200 day range. This pattern raises 
questions about whether such delayed updates are standard 
operational practice or indicative of a potential issue 
which warrants further investigation. It should be noted
that 84\% of these late updates are associated with the TLC agency
and 12\% with DSNY.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{post_closed_violin_chart.pdf}
  \caption{Post-closed \texttt{resolution\_action\_update\_date}(s) >30 days.}
  \label{fig:resolution-violin}
\end{figure}


\subsection{Accuracy and Precision:}
\label{sec:precision}
An question of precision vs. accuracy arises with the \texttt{latitude} 
and \texttt{longitude} fields. as well as with the duplicative 
\texttt{location} field. These fields are expressed as 
a 14\mbox{-}decimal number, e.g. \texttt{latitude} 40.86769186022511. 
Given that 1 degree of latitude at the equator is equal to 111.04 
kilometers, the ``1'' at the end of the decimal number represents 
approximately 1.1 nanometers (1/1,000,000,000 of a meter). (For 
reference a DNA molecule is approximately 2 nanometers in width.) The 
representation of the \texttt{latitude} and \texttt{longitude} fields
appears to be a classic case of extreme precision, but limited 
accuracy. All the \texttt{latitude}, \texttt{longitude}, and \texttt{location}
fields display this characteristic. 


\subsection{Redundant \& Duplicate fields}
\label{sec:duplicates}
During this analysis, several redundant fields were identified and should 
be examined for possible consolidation.

\paragraph{\texttt{latitude/longitude} \& \texttt{location}:} 
The \texttt{location} field is a pure concatenation of 
the \texttt{latitude} and \texttt{longitude} fields albeit with a 
comma and parentheses added. Arguably, this makes the \texttt{location} field 
more difficult to use than the individual fields. For example:  
\texttt{latitude}: ``40.768456429488'', \texttt{longitude}: ``$-$73.9575661888774'', 
form the \texttt{location}: ``(40.768456429488, $-$73.9575661888774)''.

\paragraph{\texttt{borough} \& \texttt{park\_borough}:} These two fields are fully redundant, 
as they are 100\% matches.

\paragraph{\texttt{borough} \& \texttt{taxi\_company\_borough}:} Despite 
their names, these fields are almost entirely different, with only 
a 0.05\% match. The \texttt{taxi\_company\_borough} field is 
used exclusively by the TLC, indicating a need for consultation 
with that agency to understand the differing uses. It is 
likely that that the usage of \texttt{taxi\_company\_borough} is 
quite different than that of \texttt{borough}. 

\paragraph{\texttt{agency} \& \texttt{agency\_name}:} The \texttt{agency} 
field contains the abbreviations of City agencies (e.g., NYPD, DOT, DSNY), 
while \texttt{agency\_name} contains the full names. Including both seems 
redundant, especially given the space taken up by full names in the 
CSV file and the fact that the agency abbreviations are 
easily recognized and well understood.

\paragraph{\texttt{landmark} \& \texttt{street\_name}:} Per the \href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary}, 
the \texttt{landmark} field \textit{can refer to any noteworthy location, 
including but not limited to, parks, hospitals, airports, sports facilities, 
performance spaces, etc..} However, most entries are actually 
street names, with a 62\% match to \texttt{street\_name} field. 
Even many of the non\mbox{-}matches (excluding blanks) appear to be 
a logical match, differing only in spelling or formatting 
(e.g., ``NINTH AVE'' vs. ``9 AVE''). This indicates a possible 
misuse of the \texttt{landmark} field as well as a high level of redundancy.

\paragraph{\texttt{cross\_street\_1 \& 2} \& \texttt{intersection\_street\_1 \& 2}:} These 
two street pairs are used to help identify the 
\texttt{incident address}. We found 88\% of both of these two 
street pairs ( 1 \& 2) to be duplicates. Such a high level of duplication 
raises the question of which field\mbox{-}pair should be trusted; 
and if these street pairs are used differently, how should they be treated?
It is possible that these two street pairs originated from different 
agencies that use different variable names to represent the same 
information, but no documentation exists to clarify this 
relationship. Nonetheless, these two street\mbox{-}pairs are highly redundant.


\subsection{Reducing File Size}
\label{sec:filesize}
By removing duplicate and ``near\mbox{-}duplicate'' fields, it is possible to 
shrink the file size by 39.5\%, which for this large dataset equates to 
a reduction of 1.4GB, a significant amount. A smaller file size 
means faster downloads, less storage impact, and often 
simpler data analysis efforts. Below is a list of duplicate 
and near\mbox{-}duplicate fields which we believe could be
removed from the dataset with minimal data loss. 

\begin{itemize}[left=1.5em]
    \item \textbf{\texttt{agency\_name}}: Since each row contains the \texttt{agency} 
    field, which uses clear and well understood abbreviations, we believe the
    \texttt{agency\_name} can be removed with minimal data loss.
    
    \item \textbf{\texttt{park\_borough}}: This field is a 100\% match with 
    the \texttt{borough} field and can be deleted without data loss.
    
    \item \textbf{\texttt{location}}: This field is a straight concatenation of 
    the \texttt{latitude} and \texttt{longitude} fields. It can be 
    deleted without data loss.
     
    \item \textbf{\texttt{cross\_street\_1/2}} and \textbf{\texttt{intersection\_street\_1/2}}: 
    These two street pairs both exhibit an 88\% match. While we cannot 
    confirm the accuracy of either of these street pairs, we recommend deleting the 
    \texttt{intersection\_street\_1/2} fields while acknowledging 
    the concomitant data loss.
\end{itemize}

Additionally, certain fields may not be useful for broad analysis
due to their sparse data population. This especially applies to the fields below, 
which are predominantly used by the TLC and are 99\% (or greater) blank.

\begin{quote}
\textit{[We should note that even a field that is 99\% 
blank will still contain 64,000 non\mbox{-}blank rows in this 
large (6.4 million row) dataset.}; non\mbox{-}trivial.]
\end{quote}

\begin{itemize}[left=1.5em]
    \item \texttt{taxi\_company\_borough}
    \item \texttt{road\_ramp}
    \item \texttt{vehicle\_type}
    \item \texttt{due\_date}
    \item \texttt{bridge\_highway\_direction}
    \item \texttt{bridge\_highway\_name}
    \item \texttt{bridge\_highway\_segment}
    \item \texttt{taxi\_pick\_up\_location}
\end{itemize}

\paragraph{Field Encoding:} Further reduction is possible 
by encoding selected categorical variables 
Using \texttt{complaint\_type} as an example,
instead of repeatedly storing the full text of each \texttt{complaint\_type}, 
one could use a numeric representation. For example, 
``NOISE - STREET/SIDEWALK'' could be represented by a 
numeric code (e.g. ``126''). For this dataset the 
``NOISE - STREET/SIDEWALK'' \texttt{complaint\_type} occurs 
over 300,000 times. Such encoding not only reduces file 
size, but would likely improve processing efficiency by using numeric
values instead of text strings. Moreover, standardizing 
categorical inputs prevents the inclusion of misspellings 
or unrecognized/invalid entries, ensuring cleaner, more reliable 
data. This approach is particularly useful in large datasets, where 
frequently repeated text\mbox{-}based categorical variables can 
significantly increase storage requirements. 

\subsection{Data Dictionary} 
\label{sec:datadictionary}
The \href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary} would greatly benefit from an update. We found several discrepancies between the 
Data Dictionary and the actual data, which could potentially 
mislead analytic efforts. Here are some examples:

\paragraph{Data typing:} While the NYC Open Data portal displays the data types of 
various fields (aka ``columns''), the \href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary} does 
not. For example, in the \href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary}, 
the \texttt{incident\_zip} field is specified as ``text.'' This may 
not be the most suitable data definition. Perhaps a better 
dictionary description would be  ``categorical'' with a note 
indicating that the data must contain only numeric values 
and is not subjected to arithmetic operations.

\paragraph{Domain of Legal Values:} Many of the data fields have
domains of legal values that are missing, incomplete, or inaccurate. For 
example, the \href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary} indicates 
that the \texttt{status} field takes on values of \textit{assigned, canceled, 
closed}, or \textit{pending}. However, we observed additional values 
such as \textit{in progress, started}, and \textit{unspecified}. Additionally, 
no SRs were found with a status of \textit{canceled}. Similarly, 
the Data Dictionary for the \texttt{address\_type} field lists as acceptable
values \textit{address, blockface, intersection, latlong}, and 
\textit{placename}. However, we found additional values such 
as \textit{bbl} and \textit{unrecognized}. Additionally, no 
\textit{latlong} values were observed. Other fields, such 
as \texttt{facility\_type}, \texttt{vehicle\_type}, \texttt{taxi\_pick\_up\_location},
texttt{road\_ramp}, and \texttt{city}, exhibit similar inaccuracies 
in their specified domain values.

\paragraph{Missing data fields:} As mentioned earlier, six \texttt{@computed} 
fields that are present in the data but not included in 
the \href{https://data.cityofnewyork.us/api/views/erm2-nwe9/files/b372b884-f86a-453b-ba16-1fe06ce9d212?download=true&filename=311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx}{Data Dictionary} need
to be documented and perhaps labeled as ``experimental'' 
or ``not for official use''.


\section{Government Open Data Curation Recommendations}
\label{sec:recommendations}
Building upon the insights gained from our case study of the 
NYC 311 SR data, we propose a set of data 
curation principles tailored for government released open datasets. 
These principles are designed to address the unique challenges 
and requirements observed in the curation of such datasets, 
ensuring that they remain reliable, consistent, and useful for 
public services, research, and other applications.

\subsection{Principles}
\paragraph{Principle 1: Ensuring Consistency Across Agencies and Time}
A key challenge in government released datasets is the 
harmonization of data fields across various Agencies and 
maintaining this consistency over time. To address this, agencies should adopt 
standardized naming conventions, value domains, usages, and 
formats across all datasets. Regular audits and data harmonization 
processes should be implemented to ensure consistency 
across long\mbox{-}term datasets, particularly as agency structures 
or reporting standards evolve. For instance, ensuring that 
historical datasets reflect the same field naming conventions 
over a span of years can prevent inconsistent analyses. 
These standards should be promulgated by the Open Data 
governing authority, following agreements by the relevant parties.

\paragraph{Principle 2: Guaranteeing Accuracy and Validity during Data Entry}
Data accuracy and validity are fundamental to the utility of 
open datasets. In the NYC 311 data, issues such as 
invalid zip codes and nonsensical date entries 
highlight the need for rigorous validation processes. Governments 
should establish clear protocols for data entry, ensuring that 
fields are populated with valid, legally acceptable values. To 
the maximum extent possible,the software should assist in 
ensuring data entries are accurate and valid. Some examples include:

\begin{itemize}[left=1.5em]
    \item Certain date fields should be promulgated by the software
    system versus manual entry. Critical dates should be populated
    based upon an action in the system, such as a change in status. 
       
    \item Select fields should be subjected to automated checks 
    that prevent logical errors, such as closing and opening. 
    
    \item A number of fields have clearly defined domains of legal
    values. These fields should be validated against a reference 
    dataset upon entry. 
 \end{itemize}


\paragraph{Principle 3: Optimizing Storage and Representation Efficiency}
Efficient storage and data representation are crucial for handling 
large government datasets. Our research found several duplicate and
near\mbox{-}duplicate fields in the 311 SR dataset. Eliminating 
redundant fields and encoding categorical variables can 
significantly reduce file sizes and improve the efficiency 
of data analysis. Similarly, categorical variables could be 
encoded in a standardized format to optimize storage and 
streamline analyses. Reducing file size not only saves 
storage space but also enhances the performance of queries, 
downloads,  and analysis processes.


\paragraph{Principle 4: Updated Data Dictionaries and Documentation}
Clear and accurate data documentation is essential for the effective use 
of government released datasets. Our study found several 
instances where the Data Dictionary did not accurately reflect the 
actual data. Domains of legal/allowable values should be referenced
in the Data Dictionary. Regular reviews and updates should 
occur whenever there are changes in data structure, data usage, or 
content. This practice enhances the ability of external users, 
including researchers and developers, to understand and work 
effectively with the data.

\paragraph{Principle 5: Automating Quality Assurance Processes}
Automating data validation and quality assurance processes can 
significantly improve the reliability of government datasets. Certain
unusual trends can be identified, and irregularities highlighted. 
In the NYC 311 case, issues such as large spikes in SR 
creation and closure at exactly midnight (or noon) likely 
stem from a cron script process that distorts the 
accurate capture of SR durations. By implementing 
real\mbox{-}time and event\mbox{-}driven validation tools, governments 
can both prevent such errors from entering the dataset in the first 
place, as well as highlight anomalous trends. Automated 
systems can be designed to flag inconsistencies, incorrect values, 
illogical sequences, or illogical data entries. Automation can 
also facilitate faster detection and correction of errors, reducing 
the need for manual intervention and ensuring the data 
remains clean and reliable as it is continuously updated.

\paragraph{Principle 6: Addressing Data Transparency and Accessibility}
Transparency is a core tenet of open data, and governments must ensure 
that datasets are not only accurate but also easily accessible and 
readily understandable. This involves providing clear 
metadata and making datasets available in user\mbox{-}friendly 
formats. Additionally, governments should communicate transparently about 
data quality issues, such as notifying the public when errors in SR 
fields (e.g., erroneous \texttt{closed\_ date(s)} are discovered, including
an explanation about how such issues will be resolved. By 
ensuring transparency and keeping the public informed, 
governments foster trust and promote wider use of datasets 
for research, civic engagement, and innovation.


\subsection{Actionable Steps}
While these principles are broad, actionable steps are required to 
implement them effectively. Governments can begin by investing in 
real\mbox{-}time validation systems, regular updates to data dictionaries, 
and tools for encoding and standardizing data fields. This would include
systems that automate enforcement of data validation rules 
(e.g., flagging incorrect zip codes), significantly improving data 
quality. Collaboration with city agencies and stakeholders is crucial 
to ensure that data standards, formats, and metadata meet the needs 
of diverse users, including policymakers, researchers, and the 
general public. Regular communication between agencies and external 
users can help identify recurring issues and areas where improvements 
are needed. By facilitating regular communication and collaboration 
with various stakeholders, governments can ensure data remains relevant 
and actionable for public use.


Automated Quality Assurance techniques should be employed to 
monitor the quality of the data, as measured by agreed upon
standards, such as the \% of invalid zip codes. Such tools could
capture and display the quality of the data sets as presented
on appropriate dashboards. Deviations in quality would be
automatically highlighted, alerting appropriate corrective actions.


Engagement with data scientists and statisticians is key to improving 
data curation strategies. These experts can help design and refine 
curation processes, such as creating algorithms for automatic data 
cleaning, identifying inconsistent data entries, and ensuring the 
dataset(s) structure is maintained over time. Their expertise is also 
critical in developing protocols for validating incoming data, correcting 
errors in real\mbox{-}time, and ensuring consistency across multiple data sources. 
Feedback from statisticians and data scientists, such as the issues reported 
in this paper, benefit from a clear and accessible channel to the 311 SR data 
team and other relevant City Agencies. Establishing such feedback loops 
and other opportunities for public engagement, ensures that 
recommendations for improving data curation and validation 
are not only heard and discussed, but implemented. This feedback 
mechanism would also allow ongoing dialogue between 
data users and City officials, leading to more dynamic 
and responsive data management practices.


A particularly valuable avenue for engagement is through events such as NYC 
Open Data Week, which fosters collaboration and innovation across sectors. 
Our involvement in this project stemmed from such an event, where we were 
inspired by the potential of open data to enhance public services. Governments 
should continue supporting these types of initiatives, which not only promote 
data literacy, but encourage community\mbox{-}driven improvements to datasets. 
By leveraging the expertise of data scientists, statisticians, stakeholders, 
and civic organizations, governments can ensure that their open datasets 
remain accurate, transparent, and useful for a wide range of applications.


\section{Discussion} 
\label{sec:discussion}
Our study highlights the critical role of robust data curation in
ensuring the reliability and utility of open datasets, as demonstrated
through the examination of the NYC 311 data. As these datasets are
applied across diverse fields such as public services and academic
research, inconsistencies, missing values, and formatting errors can
significantly undermine the insights derived from them. Trustworthy
machine learning systems, which rely heavily on high quality data, are
especially vulnerable to such issues. Errors in the underlying data,
such as biases or inconsistencies, can compromise both the accuracy
and fairness of predictive models, impacting real\mbox{-}world decisions in
urban planning and policy \citep{rahm2000data, geiger2020garbage}.


This research further emphasizes the necessity of harmonizing data
across City agencies, particularly resolving discrepancies in field
names, formats, usage, and definitions. Consistency in data is essential for
trustworthy machine learning, where inconsistencies across sources can
lead to distorted outcomes. This challenge is even more pronounced in
datasets that span long time periods, as changes in agency structures
or reporting standards which can introduce subtle biases and data 
distortions. Long\mbox{-}term data consistency is crucial for 
longitudinal studies and predictive modeling, as even minor 
data shifts can lead to significant deviations in model outcomes, 
as noted by \citet{rahm2000data} and\citet{borgman2012conundrum}.


Finally, the intersection of data curation and machine learning for
public policy applications opens new avenues for improving governance.
High quality data enables machine learning models to better predict
trends, allocate resources, and address service delivery issues, such
as delays in responding to 311 complaints. The COVID\mbox{-}19 pandemic
underscored the importance of real\mbox{-}time, trustworthy data in crisis
management, where the accuracy and timeliness of data\mbox{-}driven insights
were critical for public health responses. Without proper data
curation, machine learning models used during the pandemic would have
been compromised, affecting decisions on resource allocation and
service provision \citep{worby2020face, khemasuwan2021applications}.
Future efforts could focus on automating data curation processes,
particularly in real\mbox{-}time data pipelines, to ensure that the data used
in machine learning models remains accurate, clean, and reliable
\citep{chu2016data, hurbean2021open}.


\section*{Supplementary Material}
The NYC 311 SR data and code used to produce the results reported in
this paper are available [FixMe].


\bibliographystyle{jds}
\bibliography{refs}

\end{document}